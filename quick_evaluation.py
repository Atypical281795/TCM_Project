#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­é†«å•ç­”æ¨¡å‹å¿«é€Ÿè©•æ¸¬è…³æœ¬
ç°¡åŒ–ç‰ˆæœ¬ï¼Œé©åˆå¿«é€Ÿæ¸¬è©¦
"""

import sys
import os
from pathlib import Path

# plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei']
# plt.rcParams['axes.unicode_minus'] = False

# ç¢ºä¿å¯ä»¥å°å…¥ä¸»è©•æ¸¬ç³»çµ±
current_dir = Path(__file__).parent
sys.path.append(str(current_dir))

try:
    from tcm_evaluation import TCMEvaluationSystem, ModelComparator
except ImportError:
    print("éŒ¯èª¤: ç„¡æ³•å°å…¥è©•æ¸¬ç³»çµ±ï¼Œè«‹ç¢ºä¿ tcm_evaluation.py åœ¨åŒä¸€ç›®éŒ„ä¸‹")
    sys.exit(1)

def quick_evaluation():
    """å¿«é€Ÿè©•æ¸¬å‡½æ•¸"""
    
    # é…ç½®åƒæ•¸ - è«‹æ ¹æ“šä½ çš„å¯¦éš›æƒ…æ³ä¿®æ”¹
    MODEL_PATH = r"C:\Users\user\Desktop\TCM_Project\SFT_weights\Breeze_lindan_TCPM_QA"
    DATASET_PATH = "tcm_exam_question.csv"
    OUTPUT_DIR = "quick_evaluation_results"
    SAMPLE_SIZE = 500  # å¿«é€Ÿæ¸¬è©¦ç”¨è¼ƒå°æ¨£æœ¬ï¼Œè¨­ç‚º None ä½¿ç”¨å…¨éƒ¨æ•¸æ“š
    
    print("=" * 60)
    print("ä¸­é†«å•ç­”æ¨¡å‹å¿«é€Ÿè©•æ¸¬")
    print("=" * 60)
    print(f"æ¨¡å‹è·¯å¾‘: {MODEL_PATH}")
    print(f"è³‡æ–™é›†: {DATASET_PATH}")
    print(f"æ¨£æœ¬æ•¸: {SAMPLE_SIZE if SAMPLE_SIZE else 'å…¨éƒ¨'}")
    print(f"è¼¸å‡ºç›®éŒ„: {OUTPUT_DIR}")
    print()
    
    try:
        # 1. åˆå§‹åŒ–è©•æ¸¬ç³»çµ±
        print("ğŸš€ åˆå§‹åŒ–è©•æ¸¬ç³»çµ±...")
        evaluator = TCMEvaluationSystem(
            model_path=MODEL_PATH,
            dataset_path=DATASET_PATH,
            output_dir=OUTPUT_DIR
        )
        
        # 2. åŸ·è¡Œè©•æ¸¬
        print("ğŸ“ é–‹å§‹è©•æ¸¬...")
        results = evaluator.evaluate_sample(sample_size=SAMPLE_SIZE, random_seed=42)
        
        if not results:
            print("âŒ è©•æ¸¬å¤±æ•—ï¼šæ²’æœ‰ç²å¾—ä»»ä½•çµæœ")
            return
        
        # 3. è¨ˆç®—æŒ‡æ¨™
        print("ğŸ“Š è¨ˆç®—è©•æ¸¬æŒ‡æ¨™...")
        metrics = evaluator.calculate_metrics(results)
        
        # 4. ç”Ÿæˆåœ–è¡¨
        print("ğŸ“ˆ ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨...")
        evaluator.generate_visualizations(results, metrics)
        
        # 5. ä¿å­˜çµæœ
        print("ğŸ’¾ ä¿å­˜è©•æ¸¬çµæœ...")
        evaluator.save_results(results, metrics)
        
        # 6. é¡¯ç¤ºé—œéµçµæœ
        print("\n" + "=" * 50)
        print("âœ… è©•æ¸¬å®Œæˆï¼é—œéµçµæœï¼š")
        print("=" * 50)
        
        overall = metrics.get('overall', {})
        print(f"ğŸ“ˆ ç¸½æº–ç¢ºç‡: {overall.get('accuracy', 0):.2f}%")
        print(f"ğŸ“ è©•æ¸¬é¡Œæ•¸: {overall.get('total_questions', 0)}")
        print(f"ğŸ¯ æ­£ç¢ºé¡Œæ•¸: {overall.get('correct_answers', 0)}")
        print(f"ğŸ’ª å¹³å‡ä¿¡å¿ƒåˆ†æ•¸: {overall.get('avg_confidence', 0):.3f}")
        print(f"â±ï¸  å¹³å‡å›æ‡‰æ™‚é–“: {overall.get('avg_response_time', 0):.3f}ç§’")
        
        # 7. å„é¡åˆ¥è¡¨ç¾
        if 'by_category' in metrics and metrics['by_category']:
            print("\nğŸ“‹ å„é¡åˆ¥è¡¨ç¾:")
            for category, stats in metrics['by_category'].items():
                print(f"  {category}: {stats['accuracy']:.1f}% ({stats['correct']}/{stats['total']})")
        
        # 8. è¼¸å‡ºæ–‡ä»¶ä½ç½®
        print(f"\nğŸ“ è©³ç´°çµæœä¿å­˜åœ¨: {OUTPUT_DIR}/")
        print("   - detailed_results_*.csv: è©³ç´°è©•æ¸¬çµæœ")
        print("   - evaluation_report_*.json: JSONæ ¼å¼å ±å‘Š")
        print("   - summary_report_*.txt: äººé¡å¯è®€ç¸½çµ")
        print("   - evaluation_charts.png: è¦–è¦ºåŒ–åœ–è¡¨")
        
        print("\nğŸ‰ è©•æ¸¬å®Œæˆï¼")
        
    except FileNotFoundError as e:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {e}")
        print("è«‹æª¢æŸ¥æ¨¡å‹è·¯å¾‘å’Œè³‡æ–™é›†è·¯å¾‘æ˜¯å¦æ­£ç¢º")
    except Exception as e:
        print(f"âŒ è©•æ¸¬éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

def compare_models():
    """æ¨¡å‹æ¯”è¼ƒç¯„ä¾‹"""
    
    # é…ç½® - è«‹æ ¹æ“šå¯¦éš›æƒ…æ³ä¿®æ”¹
    MODEL1_PATH = r"C:\Users\user\Desktop\å°ˆé¡Œ\SFT_weights\Breeze_lindan TCPM QA"  # ä½ çš„å¾®èª¿æ¨¡å‹
    MODEL2_PATH = r"C:\Users\user\Desktop\å°ˆé¡Œ\SFT_weights\Breeze_lindan TCPM QA"  # åŸºç¤æ¨¡å‹ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
    DATASET_PATH = "tcm_exam_question.csv"
    OUTPUT_DIR = "model_comparison_results"
    SAMPLE_SIZE = 300  # æ¯”è¼ƒæ™‚å»ºè­°ä½¿ç”¨è¼ƒå°æ¨£æœ¬ä»¥ç¯€çœæ™‚é–“
    
    print("=" * 60)
    print("æ¨¡å‹æ¯”è¼ƒè©•æ¸¬ï¼ˆå«tæª¢å®šï¼‰")
    print("=" * 60)
    
    if not os.path.exists(MODEL2_PATH):
        print("âš ï¸  æœªæ‰¾åˆ°æ¯”è¼ƒæ¨¡å‹ï¼Œè«‹ä¿®æ”¹ MODEL2_PATH æˆ–è·³éæ¯”è¼ƒ")
        return
    
    try:
        # è©•æ¸¬æ¨¡å‹1
        print("ğŸ“ è©•æ¸¬æ¨¡å‹1...")
        evaluator1 = TCMEvaluationSystem(MODEL1_PATH, DATASET_PATH, OUTPUT_DIR)
        results1 = evaluator1.evaluate_sample(sample_size=SAMPLE_SIZE, random_seed=42)
        metrics1 = evaluator1.calculate_metrics(results1)
        
        # è©•æ¸¬æ¨¡å‹2
        print("ğŸ“ è©•æ¸¬æ¨¡å‹2...")
        evaluator2 = TCMEvaluationSystem(MODEL2_PATH, DATASET_PATH, OUTPUT_DIR)
        results2 = evaluator2.evaluate_sample(sample_size=SAMPLE_SIZE, random_seed=42)
        metrics2 = evaluator2.calculate_metrics(results2)
        
        # åŸ·è¡Œtæª¢å®š
        print("ğŸ“Š åŸ·è¡Œæˆå°æ¨£æœ¬tæª¢å®š...")
        t_test_result = evaluator1.paired_t_test(results1, results2)
        
        # ä¿å­˜çµæœ
        evaluator1.save_results(results1, metrics1, t_test_result, "model1_")
        evaluator2.save_results(results2, metrics2, output_prefix="model2_")
        
        # é¡¯ç¤ºæ¯”è¼ƒçµæœ
        print("\n" + "=" * 50)
        print("ğŸ“Š æ¨¡å‹æ¯”è¼ƒçµæœï¼š")
        print("=" * 50)
        
        acc1 = metrics1['overall']['accuracy']
        acc2 = metrics2['overall']['accuracy']
        
        print(f"æ¨¡å‹1æº–ç¢ºç‡: {acc1:.2f}%")
        print(f"æ¨¡å‹2æº–ç¢ºç‡: {acc2:.2f}%")
        print(f"æº–ç¢ºç‡å·®ç•°: {acc1 - acc2:+.2f}%")
        
        print(f"\ntæª¢å®šçµæœ:")
        print(f"  tçµ±è¨ˆé‡: {t_test_result['t_statistic']:.4f}")
        print(f"  på€¼: {t_test_result['p_value']:.6f}")
        print(f"  çµ±è¨ˆé¡¯è‘—: {'æ˜¯' if t_test_result['significant'] else 'å¦'}")
        print(f"  æ•ˆæ‡‰é‡(Cohen's d): {t_test_result['cohen_d']:.4f}")
        print(f"  çµæœè§£é‡‹: {t_test_result['interpretation']}")
        
    except Exception as e:
        print(f"âŒ æ¯”è¼ƒè©•æ¸¬å¤±æ•—: {e}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ä¸­é†«å•ç­”æ¨¡å‹è©•æ¸¬å·¥å…·")
    print("1. å¿«é€Ÿè©•æ¸¬")
    print("2. æ¨¡å‹æ¯”è¼ƒï¼ˆå«tæª¢å®šï¼‰")
    print("3. é€€å‡º")
    
    while True:
        choice = input("\nè«‹é¸æ“‡æ“ä½œ (1-3): ").strip()
        
        if choice == '1':
            quick_evaluation()
            break
        elif choice == '2':
            compare_models()
            break
        elif choice == '3':
            print("å†è¦‹ï¼")
            break
        else:
            print("è«‹è¼¸å…¥æœ‰æ•ˆé¸é … (1-3)")

if __name__ == "__main__":
    main()